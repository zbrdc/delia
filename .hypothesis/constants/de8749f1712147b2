# file: /home/dan/git/delia/src/delia/config.py
# hypothesis_version: 6.148.7

[0.0, 0.03, 0.3, 0.6, 0.7, 0.9, 1.1, 9.0, 10.0, 17.0, 30.0, 300.0, 100, 256, 1024, 8192, 16384, 30000, 40000, 50000, 100000, 128000, 500000, '(-a\\d+b|moe|mixtral)', '(\\d+)x(\\d+)b', '-it', '0', '1', '8192', ':latest', 'DELIA_AUTH_ENABLED', 'DELIA_BACKEND', 'DELIA_MAX_CONCURRENT', 'LLAMACPP_BASE', 'LLAMACPP_CTX_SIZE', 'LLAMACPP_MODEL', 'LLAMACPP_TYPE', 'OLLAMA_BASE', 'OLLAMA_TYPE', 'THINKING_MODEL', 'analyze', 'assistant', 'available', 'base', 'chat', 'circuit_open', 'code', 'code-gemma', 'code-llama', 'codegemma', 'codellama', 'coder', 'codestral', 'consecutive_failures', 'critique', 'deepseek', 'deepseek-coder', 'deepseek-v2', 'deepseek-v3', 'falcon', 'falcon2', 'false', 'foundation', 'gemma', 'gemma-2', 'gemma2', 'generate', 'gpt', 'gpt-j', 'gpt-neo', 'gpt-neox', 'gpt2', 'granite-code', 'hermes', 'instruct', 'last_error', 'llama', 'llama-3', 'llama2', 'llama3', 'llamacpp', 'local', 'magicoder', 'meta-llama', 'mistral', 'mistral-nemo', 'mixtral', 'moe', 'nemotron', 'nous-hermes', 'ollama', 'olmo3:7b-think', 'orca', 'orca2', 'phi', 'phi-2', 'phi-3', 'phi-4', 'phi2', 'phi3', 'phi4', 'plan', 'pretrain', 'quick', 'qwen', 'qwen2', 'qwen2.5', 'qwen2.5-coder:14b', 'qwen3', 'qwen3:14b', 'qwen3:30b-a3b', 'raw', 'remote', 'review', 'rlhf', 'safe_context_kb', 'stable-code', 'starcoder', 'starcoder2', 'starcoderbase', 'thinking', 'timeout', 'true', 'wizard-coder', 'wizardcoder', 'yes', 'yi', 'yi-1.5', 'yi-coder']